{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a82f216c-85f4-4c0a-8524-7e1c8cf1bdf9",
   "metadata": {},
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "#In this assignment, I try to scrape data from transfermakt.com. Specifically, I will scrape information of Turkish Superleague player's information for\n",
    "#every team.\n",
    "\n",
    "\n",
    "# Define headers to simulate a browser request\n",
    "headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "def scrape_team_players(team_url, club_name):\n",
    "\n",
    "\n",
    "    \n",
    "    response = requests.get(team_url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        \n",
    "        player_table = soup.find('table', class_='items')\n",
    "\n",
    "    \n",
    "        player_data = []\n",
    "\n",
    "        for player_row in player_table.find_all('tr')[1:]: \n",
    "            player_cells = player_row.find_all(['th', 'td'])\n",
    "            player_info = [cell.text.strip() for cell in player_cells]\n",
    "\n",
    "            \n",
    "            if not player_info[0]:\n",
    "                continue\n",
    "\n",
    "            # Clean up additional information for the same player\n",
    "            player_info = [info for info in player_info if info]\n",
    "\n",
    "            \n",
    "            player_data.append([club_name] + player_info)\n",
    "\n",
    "        # number of columns\n",
    "        num_columns = max(len(row) for row in player_data)\n",
    "\n",
    "        # Create column names based on the number of columns\n",
    "        columns = ['Club'] + [f'Column_{i + 1}' for i in range(num_columns - 1)]\n",
    "\n",
    "        # Create a Pandas DataFrame from the extracted data\n",
    "        df = pd.DataFrame(player_data, columns=columns)\n",
    "\n",
    "        return df\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: Unable to fetch the page (Status Code: {response.status_code})\")\n",
    "        return None\n",
    "\n",
    "# URL of the SÃ¼per Lig\n",
    "super_lig_url = 'https://www.transfermarkt.com.tr/super-lig/startseite/wettbewerb/TR1'\n",
    "\n",
    "\n",
    "response = requests.get(super_lig_url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    \n",
    "    teams_table = soup.find('table', class_='items')\n",
    "\n",
    "    # Extract team URLs and names from the table\n",
    "    team_data = [(a['href'], a.text.strip()) for a in teams_table.select('td.hauptlink a[href^=\"/\"]')]\n",
    "\n",
    "    \n",
    "    all_players_df = pd.DataFrame()\n",
    "\n",
    "    # Loop through each team URL and scrape player information\n",
    "    for team_url, club_name in team_data:\n",
    "        team_url = 'https://www.transfermarkt.com.tr' + team_url\n",
    "        team_players_df = scrape_team_players(team_url, club_name)\n",
    "\n",
    "        # Check if the DataFrame is not empty before concatenating\n",
    "        if team_players_df is not None and not team_players_df.empty:\n",
    "            all_players_df = pd.concat([all_players_df, team_players_df], ignore_index=True)\n",
    "\n",
    "    \n",
    "    display(all_players_df)\n",
    "\n",
    "else:\n",
    "    print(f\"Error: Unable to fetch the page (Status Code: {response.status_code})\")\n",
    "\n",
    "\n",
    "\n",
    "#Further data cleaning\n",
    "columns = ['Club', 'No', 'Name1', 'Name', 'Position', 'Age', 'Market_Value']\n",
    "all_players_df.columns = columns\n",
    "del all_players_df[\"Name1\"]\n",
    "all_players_df= all_players_df.dropna(how=\"any\")\n",
    "all_players_df=all_players_df.reset_index(drop=True)\n",
    "\n",
    "all_players_df.to_csv('all_superleague_players.csv', index=False)\n",
    "all_players_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3777c61-4798-45dd-82ea-2df20d01809c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f20248-3200-4fe2-9f2e-51d5444708ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
